import flax
import jax
import jax.numpy as jnp
from brax.training.types import Params


@flax.struct.dataclass
class MBPPOParams:
    """Contains training state for the learner."""

    policy: Params
    value: Params
    cost_value: Params
    model: Params


def compute_gae(
    truncation: jnp.ndarray,
    termination: jnp.ndarray,
    rewards: jnp.ndarray,
    values: jnp.ndarray,
    bootstrap_value: jnp.ndarray,
    lambda_: float = 1.0,
    discount: float = 0.99,
):
    """Calculates the Generalized Advantage Estimation (GAE).

    Args:
      truncation: A float32 tensor of shape [T, B] with truncation signal.
      termination: A float32 tensor of shape [T, B] with termination signal.
      rewards: A float32 tensor of shape [T, B] containing rewards generated by
        following the behaviour policy.
      values: A float32 tensor of shape [T, B] with the value function estimates
        wrt. the target policy.
      bootstrap_value: A float32 of shape [B] with the value function estimate at
        time T.
      lambda_: Mix between 1-step (lambda_=0) and n-step (lambda_=1). Defaults to
        lambda_=1.
      discount: TD discount.

    Returns:
      A float32 tensor of shape [T, B]. Can be used as target to
        train a baseline (V(x_t) - vs_t)^2.
      A float32 tensor of shape [T, B] of advantages.
    """

    truncation_mask = 1 - truncation
    # Append bootstrapped value to get [v1, ..., v_t+1]
    values_t_plus_1 = jnp.concatenate(
        [values[1:], jnp.expand_dims(bootstrap_value, 0)], axis=0
    )
    deltas = rewards + discount * (1 - termination) * values_t_plus_1 - values
    deltas *= truncation_mask

    acc = jnp.zeros_like(bootstrap_value)
    vs_minus_v_xs = []

    def compute_vs_minus_v_xs(carry, target_t):
        lambda_, acc = carry
        truncation_mask, delta, termination = target_t
        acc = delta + discount * (1 - termination) * truncation_mask * lambda_ * acc
        return (lambda_, acc), (acc)

    (_, _), (vs_minus_v_xs) = jax.lax.scan(
        compute_vs_minus_v_xs,
        (lambda_, acc),
        (truncation_mask, deltas, termination),
        length=int(truncation_mask.shape[0]),
        reverse=True,
    )
    # Add V(x_s) to get v_s.
    vs = jnp.add(vs_minus_v_xs, values)

    vs_t_plus_1 = jnp.concatenate([vs[1:], jnp.expand_dims(bootstrap_value, 0)], axis=0)
    advantages = (
        rewards + discount * (1 - termination) * vs_t_plus_1 - values
    ) * truncation_mask
    return jax.lax.stop_gradient(vs), jax.lax.stop_gradient(advantages)


def make_losses(
    ppo_network,
    clipping_epsilon,
    entropy_cost,
    reward_scaling,
    discounting,
    gae_lambda,
    normalize_advantage,
    cost_scaling,
    safety_budget,
    safety_discounting,
    safety_gae_lambda,
):
    def _neg_log_posterior(
        predicted_outputs: jax.Array,
        predicted_stds: jax.Array,
        target_outputs: jax.Array,
        learn_std: bool,
    ) -> jax.Array:
        def _nll(
            predicted_outputs: jax.Array,
            predicted_stds: jax.Array,
            target_outputs: jax.Array,
        ) -> jax.Array:
            if learn_std:
                log_prob = jax.scipy.stats.norm.logpdf(
                    target_outputs, loc=predicted_outputs, scale=predicted_stds
                )
                return -jnp.mean(log_prob)
            else:
                loss = jnp.square(target_outputs - predicted_outputs).mean()
                return loss

        nll = jax.vmap(jax.vmap(_nll))(
            predicted_outputs, predicted_stds, target_outputs
        )
        neg_log_post = nll.mean()
        return neg_log_post

    def compute_model_loss(
        model_params,
        normalizer_params,
        data,
        key,
        learn_std,
    ):
        model_apply = ppo_network.model_network.apply
        (
            (next_obs_pred, reward_pred, cost_pred),
            (next_obs_std, reward_std, cost_std),
        ) = model_apply(normalizer_params, model_params, data.observation, data.action)
        expand = lambda x: jnp.tile(
            x[None], (next_obs_pred.shape[0],) + (1,) * (x.ndim)
        )
        next_obs_target = expand(data.next_observation)
        next_obs_loss = _neg_log_posterior(
            next_obs_pred, next_obs_std, next_obs_target, learn_std
        )
        current_reward_target = expand(data.reward)
        reward_loss = _neg_log_posterior(
            reward_pred, reward_std, current_reward_target, learn_std
        )
        cost_loss = 0.0
        cost_target = None
        if "state_extras" in data.extras and "cost" in data.extras["state_extras"]:
            cost_target = expand(data.extras["state_extras"]["cost"])
            cost_loss = _neg_log_posterior(cost_pred, cost_std, cost_target, learn_std)
        # FIXME: (manu) test zero loss change back!
        total_loss = next_obs_loss + reward_loss + 0 * cost_loss
        # Compute MSE for monitoring
        obs_mse = jnp.mean(jnp.square(next_obs_pred - next_obs_target))
        reward_mse = jnp.mean(jnp.square(reward_pred - current_reward_target))
        # FIXME: (manu) for test compte loss with mse only
        total_loss = obs_mse + reward_mse
        cost_mse = 0.0
        if cost_target is not None:
            cost_mse = jnp.mean(jnp.square(cost_pred - cost_target))
        aux = {
            "model_loss": total_loss,
            "obs_mse": obs_mse,
            "reward_mse": reward_mse,
            "cost_mse": cost_mse,
            "next_obs_loss": next_obs_loss,
            "reward_loss": reward_loss,
            "cost_loss": cost_loss,
        }
        return total_loss, aux

    def compute_policy_loss(
        policy_params,
        value_params,
        normalizer_params,
        data,
        key,
    ):
        parametric_action_distribution = ppo_network.parametric_action_distribution
        policy_apply = ppo_network.policy_network.apply
        value_apply = ppo_network.value_network.apply
        data = jax.tree_util.tree_map(lambda x: jnp.swapaxes(x, 0, 1), data)
        policy_logits = policy_apply(normalizer_params, policy_params, data.observation)
        baseline = value_apply(normalizer_params, value_params, data.observation)
        terminal_obs = jax.tree_util.tree_map(lambda x: x[-1], data.next_observation)
        bootstrap_value = value_apply(normalizer_params, value_params, terminal_obs)
        rewards = data.reward * reward_scaling
        truncation = data.extras["state_extras"]["truncation"]
        termination = (1 - data.discount) * (1 - truncation)
        raw_action = data.extras["policy_extras"]["raw_action"]
        target_log_probs = parametric_action_distribution.log_prob(
            policy_logits,
            raw_action,  # Use fixed shape
        )
        behavior_log_probs = data.extras["policy_extras"]["log_prob"]
        _, advantages = compute_gae(
            truncation=truncation,
            termination=termination,
            rewards=rewards,
            values=baseline,
            bootstrap_value=bootstrap_value,
            lambda_=gae_lambda,
            discount=discounting,
        )
        if normalize_advantage:
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        rho_s = jnp.exp(target_log_probs - behavior_log_probs)
        surrogate1 = rho_s * advantages
        surrogate2 = (
            jnp.clip(rho_s, 1 - clipping_epsilon, 1 + clipping_epsilon) * advantages
        )
        policy_loss = -jnp.mean(jnp.minimum(surrogate1, surrogate2))
        entropy = jnp.mean(parametric_action_distribution.entropy(policy_logits, key))
        entropy_loss = -entropy_cost * entropy
        aux = {
            "policy_loss": policy_loss,
            "entropy_loss": entropy_loss,
        }

        total_loss = policy_loss + entropy_loss
        return total_loss, aux

    def compute_value_loss(params, normalizer_params, data):
        value_apply = ppo_network.value_network.apply
        data = jax.tree_util.tree_map(lambda x: jnp.swapaxes(x, 0, 1), data)
        baseline = value_apply(normalizer_params, params, data.observation)
        terminal_obs = jax.tree_util.tree_map(lambda x: x[-1], data.next_observation)
        bootstrap_value = value_apply(normalizer_params, params, terminal_obs)
        rewards = data.reward * reward_scaling
        truncation = data.extras["state_extras"]["truncation"]
        termination = (1 - data.discount) * (1 - truncation)
        vs, _ = compute_gae(
            truncation=truncation,
            termination=termination,
            rewards=rewards,
            values=baseline,
            bootstrap_value=bootstrap_value,
            lambda_=gae_lambda,
            discount=discounting,
        )
        v_error = vs - baseline
        v_loss = jnp.mean(v_error**2) * 0.25
        return v_loss, {"v_loss": v_loss}

    def compute_cost_value_loss(params, normalizer_params, data):
        cost_value_apply = ppo_network.cost_value_network.apply
        data = jax.tree_util.tree_map(lambda x: jnp.swapaxes(x, 0, 1), data)
        cost = data.extras["state_extras"]["cost"] * cost_scaling
        cost_baseline = cost_value_apply(normalizer_params, params, data.observation)
        terminal_obs = jax.tree_util.tree_map(lambda x: x[-1], data.next_observation)
        cost_bootstrap = cost_value_apply(normalizer_params, params, terminal_obs)
        truncation = data.extras["state_extras"]["truncation"]
        termination = (1 - data.discount) * (1 - truncation)
        vcs, _ = compute_gae(
            truncation=truncation,
            termination=termination,
            rewards=cost,
            values=cost_baseline,
            bootstrap_value=cost_bootstrap,
            lambda_=safety_gae_lambda,
            discount=safety_discounting,
        )
        cost_error = vcs - cost_baseline
        cost_v_loss = jnp.mean(cost_error**2) * 0.25
        return cost_v_loss, {"cost_v_loss": cost_v_loss}

    return (
        compute_model_loss,
        compute_policy_loss,
        compute_value_loss,
        compute_cost_value_loss,
    )
