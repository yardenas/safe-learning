# @package _global_
defaults:
  - override /environment: rccar_real
  - override /agent: mbpo
  - override /agent/penalizer: lagrangian
  - _self_

environment:
  action_delay: 1
  observation_delay: 0
  sliding_window: 5
  dt: 0.03333333
  sample_init_pose: true

training:
  num_envs: 1
  num_timesteps: 50000
  episode_length: 250
  safe: true
  train_domain_randomization: false
  eval_domain_randomization: false
  safety_budget: 5.0
  wandb_id: vahll47z-fixed
  render: false

agent:
  batch_size: 256
  min_replay_size: 500
  max_replay_size: 1048576
  policy_hidden_layer_sizes: [64, 64]
  critic_grad_updates_per_step: 10
  model_grad_updates_per_step: 50
  num_model_rollouts: 100000
  learning_rate: 3e-4
  critic_learning_rate: 3e-4
  model_learning_rate: 3e-4
  use_termination: false
  pessimism: 30
  # Use negative optimism so that the reward is pessimistic w.r.t uncertianty
  # following MOPO paper.
  optimism: -100.
  safety_filter: null
  offline: true
  penalizer:
    lagrange_multiplier: 0.1
    penalty_multiplier: 0.001
    penalty_multiplier_factor: 1e-4