# @package _global_
defaults:
  - override /environment: go_to_goal_easy
  - override /agent: sbsrl
  - override /agent/penalizer: multiaug_lagrangian
  - _self_

training:
  num_timesteps: 2500000
  train_domain_randomization: false
  eval_domain_randomization: false
  safe: true
  safety_budget: 25
  action_repeat: 4
  num_envs: 20
  num_evals: 15
  wandb_id: 7v4m2d6t #,h9u3sf0z,twij52il,uzw1fq1a,25ltbto0

agent:
  policy_hidden_layer_sizes: [256, 256, 256]
  value_hidden_layer_sizes: [512, 512]
  activation: swish
  batch_size: 256
  max_replay_size: 4194304
  critic_grad_updates_per_step: 10
  model_grad_updates_per_step: 50
  num_model_rollouts: 50000
  discounting: 0.99
  safety_discounting: 0.999
  normalize_budget: false
  reward_scaling: 1
  cost_scaling: 1
  uncertainty_constraint: true
  uncertainty_epsilon: 150 #0
  model_to_real_data_ratio: 0.5
  use_mean_critic: false
  penalizer:
    lagrange_multiplier: 0.1
    penalty_multiplier: 0.001
    penalty_multiplier_factor: 1e-4
