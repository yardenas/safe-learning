name: awac_mpc

learning_rate: 3e-4
critic_learning_rate: 3e-4
discounting: 0.99
batch_size: 256
normalize_observations: true
reward_scaling: 1.0
tau: 0.005
min_replay_size: 8192
max_replay_size: 1000000
sim_prefill_steps: 0
critic_pretrain_ratio: 0.0
grad_updates_per_step: 512
num_critic_updates_per_actor_update: 1
deterministic_eval: true

rollout_length: 1
awac_lambda: 1.0
max_weight: 100.0
actor_update_source: planner_online
planner_rollout_steps: 1

policy_hidden_layer_sizes: [128, 128]
value_hidden_layer_sizes: [512, 512]
activation: swish
use_bro: true
n_critics: 2
n_heads: 1

controller_name: tree
controller_kwargs:
  num_samples: 256
  horizon: 100
  action_repeat: 10
  use_policy: true
  use_critic: true
  planner: sir
  action_noise_std: 0.3
  mode: resample
  gae_lambda: 0.0
  temperature: 0.5
  iterations: 1
