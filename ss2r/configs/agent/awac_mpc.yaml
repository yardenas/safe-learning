name: awac_mpc

learning_rate: 3e-4
critic_learning_rate: 3e-4
discounting: 0.99
batch_size: 256
normalize_observations: true
reward_scaling: 1.0
tau: 0.005
min_replay_size: 8192
max_replay_size: 1000000
grad_updates_per_step: 512
num_critic_updates_per_actor_update: 1
deterministic_eval: true

rollout_length: 1
mpo_eta: 1.0
mpo_num_action_samples: 16
actor_update_source: planner_online

policy_hidden_layer_sizes: [128, 128]
value_hidden_layer_sizes: [512, 512]
activation: swish
use_bro: true
n_critics: 2
n_heads: 1

controller_name: tree
controller_kwargs:
  num_samples: 128
  horizon: 25
  zoh_steps: 5
  use_policy: true
  use_critic: true
  action_noise_std: 0.3
  gae_lambda: 1.0
  temperature: 0.8
  iterations: 2
