defaults:
  - cost_robustness: null
  - reward_robustness: null
  - propagation: null
  - data_collection: step
  - penalizer: multi_lagrangian

name: sbsrl
learning_rate: 3e-4
critic_learning_rate: 3e-4
model_learning_rate: 3e-4
alpha_learning_rate: 3e-4
init_alpha: 1.
discounting: 0.99
min_alpha: 0.
safety_discounting: 0.99
sigma_discounting: 0.99
batch_size: 256
sac_batch_size: 256
normalize_observations: True
normalize_disagreement: True
reward_scaling: 1.0
cost_scaling: 1.0
sigma_scaling: 1.0
tau: 0.005
min_replay_size: 8192
max_replay_size: 1000000
model_grad_updates_per_step: 512
critic_grad_updates_per_step: 512
model_to_real_data_ratio: 1. # Use only model generated data
num_critic_updates_per_actor_update: 1
deterministic_eval: true
policy_hidden_layer_sizes: [128, 128]
value_hidden_layer_sizes: [512, 512]
model_hidden_layer_sizes: [256, 256, 256]
activation: swish
use_bro: true
normalize_budget: true
reset_on_eval: true
store_buffer: false
n_critics: 2
n_heads: 1
model_ensemble_size: 5
embedding_dim: 32
unroll_length: 1
num_model_rollouts: 400
reward_pessimism: 0.
cost_pessimism: 0.
model_propagation: all
offline: false
save_sooper_backup: false
flip_uncertainty_constraint: false
learn_from_scratch: false
target_entropy: null
pessimistic_q: false
training_step_fn: on_policy
uncertainty_constraint: false
uncertainty_epsilon: 0.0
use_mean_critic: false
load_disagreement_normalizer: false
pessimistic_cost: false