defaults:
  - data_collection: episodic

name: mb_ppo

actor_critic_learning_rate: 1e-3
model_learning_rate: 1e-3
entropy_cost: 0.05
discounting: 0.995
safety_discounting: 0.9
unroll_length: 10
batch_size: 1024
num_minibatches: 32
num_updates_per_batch: 16
ppo_updates_per_step: 100
model_updates_per_step: 100
min_replay_size: 8192
max_replay_size: 1048576
normalize_observations: true
reward_scaling: 10.0
cost_scaling: 1.0
clipping_epsilon: 0.3
gae_lambda: 0.95
deterministic_eval: true
normalize_advantage: true
model_hidden_layer_sizes: [512, 512]
policy_hidden_layer_sizes: [64, 64, 64]
value_hidden_layer_sizes: [64, 64, 64]
activation: swish
max_grad_norm: 1.0
use_bro: false
n_ensemble: 5
ensemble_selection: mean
learn_std: false
normalize_budget: true
reset_on_eval: true
pretrain_model: true
pretrain_epochs: 1000
pretrain_num_samples: 1000000
store_buffer: false
